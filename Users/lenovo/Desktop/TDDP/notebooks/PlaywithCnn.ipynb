{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e15015a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Utils')\n",
    "import configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90b5e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7, dropout_prob=0.5):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64,128,3,1,1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128,128,3,1,1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128,256,3,1,1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc1 = nn.Linear(256*28*28, 512)\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "        self.fc3 = nn.Linear(256,num_classes)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7734f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(0.2),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(0.2,0.2,0.1),\n",
    "    transforms.RandomAffine(0, translate=(0.1,0.1), scale=(0.9,1.1)),\n",
    "    transforms.RandomPerspective(0.1,0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Validation / Test (no augmentation)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_dataset = datasets.ImageFolder(configs.MULTIVIEW_TRAIN_DIR, transform=transform_train)\n",
    "test_dataset = datasets.ImageFolder(configs.MULTIVIEW_TEST_DIR, transform=transform_test)\n",
    "\n",
    "# Train / Validation Split\n",
    "train_size = int(0.8*len(train_dataset))\n",
    "val_size = len(train_dataset)-train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_dataset,[train_size,val_size])\n",
    "\n",
    "val_dataset_no_aug = datasets.ImageFolder(configs.MULTIVIEW_TRAIN_DIR, transform=transform_test)\n",
    "val_subset_no_aug = Subset(val_dataset_no_aug, val_subset.indices)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_subset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_subset_no_aug, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52a751df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, running_corrects, total = 0, 0, 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs,1)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()*inputs.size(0)\n",
    "        running_corrects += torch.sum(preds==labels.data)\n",
    "        total += inputs.size(0)\n",
    "    return running_loss/total, running_corrects.double()/total\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, running_corrects, total = 0,0,0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs,1)\n",
    "            loss = criterion(outputs,labels)\n",
    "            running_loss += loss.item()*inputs.size(0)\n",
    "            running_corrects += torch.sum(preds==labels.data)\n",
    "            total += inputs.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return running_loss/total, running_corrects.double()/total, all_labels, all_preds\n",
    "\n",
    "def train_model_with_history(model, train_loader, val_loader, criterion, optimizer, scheduler=None, \n",
    "                             num_epochs=50, patience=10, device=None):\n",
    "    history = {\"train_loss\":[],\"train_acc\":[],\"val_loss\":[],\"val_acc\":[]}\n",
    "    best_acc = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    patience_counter = 0\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc.item())\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc.item())\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "        if val_acc>best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            patience_counter=0\n",
    "        else:\n",
    "            patience_counter+=1\n",
    "            if patience_counter>=patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n",
    "              f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b7ce9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs,1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    prec = precision_score(all_labels, all_preds, average='weighted')\n",
    "    print(f\"Accuracy={acc:.4f}, Balanced Acc={bal_acc:.4f}, F1={f1:.4f}, Precision={prec:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds))\n",
    "    return all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efd45ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss\"); plt.title(\"Loss Curve\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history[\"train_acc\"], label=\"Train Acc\")\n",
    "    plt.plot(history[\"val_acc\"], label=\"Val Acc\")\n",
    "    plt.xlabel(\"Epochs\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy Curve\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b988b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-09 22:10:29,292] A new study created in memory with name: no-name-436bdb0d-4c97-4781-a689-86f87533e7db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=2.0986, Train Acc=0.1683, Val Loss=1.9425, Val Acc=0.1688\n",
      "Epoch 2: Train Loss=1.9666, Train Acc=0.1827, Val Loss=1.9656, Val Acc=0.0592\n",
      "Epoch 3: Train Loss=1.9745, Train Acc=0.1710, Val Loss=1.9425, Val Acc=0.2334\n",
      "Epoch 4: Train Loss=1.9670, Train Acc=0.1548, Val Loss=1.9464, Val Acc=0.2639\n",
      "Epoch 5: Train Loss=1.9620, Train Acc=0.1952, Val Loss=1.9491, Val Acc=0.0592\n",
      "Epoch 6: Train Loss=1.9503, Train Acc=0.1988, Val Loss=1.9424, Val Acc=0.2639\n",
      "Epoch 7: Train Loss=1.9475, Train Acc=0.2401, Val Loss=1.9398, Val Acc=0.2639\n",
      "Epoch 8: Train Loss=1.9340, Train Acc=0.2267, Val Loss=1.8832, Val Acc=0.2639\n",
      "Epoch 9: Train Loss=1.9002, Train Acc=0.2621, Val Loss=1.8682, Val Acc=0.3609\n",
      "Epoch 10: Train Loss=1.8532, Train Acc=0.3066, Val Loss=1.8609, Val Acc=0.2873\n",
      "Epoch 11: Train Loss=1.8014, Train Acc=0.3205, Val Loss=1.6833, Val Acc=0.4075\n",
      "Epoch 12: Train Loss=1.7644, Train Acc=0.3353, Val Loss=1.6645, Val Acc=0.4578\n",
      "Epoch 13: Train Loss=1.7301, Train Acc=0.3465, Val Loss=1.5594, Val Acc=0.4614\n",
      "Epoch 14: Train Loss=1.6845, Train Acc=0.3357, Val Loss=1.7092, Val Acc=0.4381\n",
      "Epoch 15: Train Loss=1.6601, Train Acc=0.3452, Val Loss=1.5878, Val Acc=0.4829\n",
      "Epoch 16: Train Loss=1.6405, Train Acc=0.3411, Val Loss=1.4361, Val Acc=0.4470\n",
      "Epoch 17: Train Loss=1.6006, Train Acc=0.3577, Val Loss=1.4621, Val Acc=0.4704\n",
      "Epoch 18: Train Loss=1.5932, Train Acc=0.3945, Val Loss=1.4156, Val Acc=0.4201\n",
      "Epoch 19: Train Loss=1.5534, Train Acc=0.3941, Val Loss=1.3788, Val Acc=0.4811\n",
      "Epoch 20: Train Loss=1.5528, Train Acc=0.3972, Val Loss=1.3458, Val Acc=0.4686\n",
      "Epoch 21: Train Loss=1.5457, Train Acc=0.3914, Val Loss=1.3593, Val Acc=0.4614\n",
      "Epoch 22: Train Loss=1.5151, Train Acc=0.4013, Val Loss=1.3504, Val Acc=0.3986\n",
      "Epoch 23: Train Loss=1.5318, Train Acc=0.4093, Val Loss=1.3136, Val Acc=0.4937\n",
      "Epoch 24: Train Loss=1.4615, Train Acc=0.4201, Val Loss=1.2604, Val Acc=0.4668\n",
      "Epoch 25: Train Loss=1.4594, Train Acc=0.4232, Val Loss=1.2861, Val Acc=0.4811\n",
      "Epoch 26: Train Loss=1.4522, Train Acc=0.4273, Val Loss=1.2713, Val Acc=0.4865\n",
      "Epoch 27: Train Loss=1.4666, Train Acc=0.4156, Val Loss=1.2795, Val Acc=0.4668\n",
      "Epoch 28: Train Loss=1.4455, Train Acc=0.4125, Val Loss=1.2249, Val Acc=0.4704\n",
      "Epoch 29: Train Loss=1.4071, Train Acc=0.4470, Val Loss=1.2497, Val Acc=0.4919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-10 03:34:31,245] Trial 0 finished with value: 0.4991023339317774 and parameters: {'lr': 0.0005878148120866139, 'dropout': 0.37988859792925234, 'weight_decay': 0.0003158670518170544}. Best is trial 0 with value: 0.4991023339317774.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss=1.3980, Train Acc=0.4412, Val Loss=1.1695, Val Acc=0.4991\n",
      "Epoch 1: Train Loss=2.1277, Train Acc=0.1939, Val Loss=1.9343, Val Acc=0.2962\n",
      "Epoch 2: Train Loss=1.9684, Train Acc=0.1881, Val Loss=1.9331, Val Acc=0.3088\n",
      "Epoch 3: Train Loss=1.9571, Train Acc=0.1697, Val Loss=1.9449, Val Acc=0.2639\n",
      "Epoch 4: Train Loss=1.9534, Train Acc=0.1854, Val Loss=1.9459, Val Acc=0.2262\n",
      "Epoch 5: Train Loss=1.9514, Train Acc=0.2208, Val Loss=1.9426, Val Acc=0.2334\n",
      "Epoch 6: Train Loss