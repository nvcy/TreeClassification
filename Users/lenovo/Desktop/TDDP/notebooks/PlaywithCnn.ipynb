{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e15015a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Utils')\n",
    "import configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90b5e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7, dropout_prob=0.5):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64,128,3,1,1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128,128,3,1,1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128,256,3,1,1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc1 = nn.Linear(256*28*28, 512)\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "        self.fc3 = nn.Linear(256,num_classes)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7734f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(0.2),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(0.2,0.2,0.1),\n",
    "    transforms.RandomAffine(0, translate=(0.1,0.1), scale=(0.9,1.1)),\n",
    "    transforms.RandomPerspective(0.1,0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Validation / Test (no augmentation)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Grayscale(1),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_dataset = datasets.ImageFolder(configs.MULTIVIEW_TRAIN_DIR, transform=transform_train)\n",
    "test_dataset = datasets.ImageFolder(configs.MULTIVIEW_TEST_DIR, transform=transform_test)\n",
    "\n",
    "# Train / Validation Split\n",
    "train_size = int(0.8*len(train_dataset))\n",
    "val_size = len(train_dataset)-train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_dataset,[train_size,val_size])\n",
    "\n",
    "val_dataset_no_aug = datasets.ImageFolder(configs.MULTIVIEW_TRAIN_DIR, transform=transform_test)\n",
    "val_subset_no_aug = Subset(val_dataset_no_aug, val_subset.indices)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_subset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_subset_no_aug, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a751df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss, running_corrects, total = 0, 0, 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs,1)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()*inputs.size(0)\n",
    "        running_corrects += torch.sum(preds==labels.data)\n",
    "        total += inputs.size(0)\n",
    "    return running_loss/total, running_corrects.double()/total\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, running_corrects, total = 0,0,0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs,1)\n",
    "            loss = criterion(outputs,labels)\n",
    "            running_loss += loss.item()*inputs.size(0)\n",
    "            running_corrects += torch.sum(preds==labels.data)\n",
    "            total += inputs.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return running_loss/total, running_corrects.double()/total, all_labels, all_preds\n",
    "\n",
    "def train_model_with_history(model, train_loader, val_loader, criterion, optimizer, scheduler=None, \n",
    "                             num_epochs=50, patience=10, device=None):\n",
    "    history = {\"train_loss\":[],\"train_acc\":[],\"val_loss\":[],\"val_acc\":[]}\n",
    "    best_acc = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    patience_counter = 0\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc.item())\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc.item())\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "        if val_acc>best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            patience_counter=0\n",
    "        else:\n",
    "            patience_counter+=1\n",
    "            if patience_counter>=patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n",
    "              f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b7ce9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs,1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    prec = precision_score(all_labels, all_preds, average='weighted')\n",
    "    print(f\"Accuracy={acc:.4f}, Balanced Acc={bal_acc:.4f}, F1={f1:.4f}, Precision={prec:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds))\n",
    "    return all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efd45ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss\"); plt.title(\"Loss Curve\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history[\"train_acc\"], label=\"Train Acc\")\n",
    "    plt.plot(history[\"val_acc\"], label=\"Val Acc\")\n",
    "    plt.xlabel(\"Epochs\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy Curve\"); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b988b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Trial 1/10: lr=0.00601, dropout=0.419, weight_decay=0.000244\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define search space manually\n",
    "LR_RANGE = (1e-4, 1e-2)\n",
    "DROPOUT_RANGE = (0.3, 0.6)\n",
    "WEIGHT_DECAY_RANGE = (1e-5, 1e-3)\n",
    "\n",
    "def random_search(n_trials=10):\n",
    "    best_acc = 0.0\n",
    "    best_params = {}\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        # Randomly sample hyperparameters\n",
    "        lr = 10 ** np.random.uniform(np.log10(LR_RANGE[0]), np.log10(LR_RANGE[1]))\n",
    "        dropout = np.random.uniform(*DROPOUT_RANGE)\n",
    "        weight_decay = 10 ** np.random.uniform(np.log10(WEIGHT_DECAY_RANGE[0]), np.log10(WEIGHT_DECAY_RANGE[1]))\n",
    "\n",
    "        print(f\"\\nüîé Trial {trial+1}/{n_trials}: lr={lr:.5f}, dropout={dropout:.3f}, weight_decay={weight_decay:.6f}\")\n",
    "\n",
    "        # Subsample training data (30%)\n",
    "        subset_size = int(0.3 * len(train_subset))\n",
    "        subset_indices = np.random.choice(train_subset.indices, subset_size, replace=False)\n",
    "        trial_subset = torch.utils.data.Subset(train_dataset, subset_indices)\n",
    "        trial_loader = DataLoader(trial_subset, batch_size=16, shuffle=True, num_workers=2)\n",
    "\n",
    "        # Build model\n",
    "        model = DeepCNN(num_classes=7, dropout_prob=dropout)\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        # Compute class weights\n",
    "        trial_labels = [train_dataset.targets[i] for i in subset_indices]\n",
    "        classes = np.unique(trial_labels)\n",
    "        class_weights = compute_class_weight('balanced', classes=classes, y=trial_labels)\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "\n",
    "        # Train for fewer epochs just to test hyperparams quickly\n",
    "        model, history = train_model_with_history(\n",
    "            model, trial_loader, val_loader,\n",
    "            criterion, optimizer, scheduler,\n",
    "            num_epochs=8,\n",
    "            device=device,\n",
    "            patience=3\n",
    "        )\n",
    "\n",
    "        val_acc = max(history[\"val_acc\"])\n",
    "        print(f\"‚úÖ Trial {trial+1} validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        # Keep track of best\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_params = {\"lr\": lr, \"dropout\": dropout, \"weight_decay\": weight_decay}\n",
    "\n",
    "    print(\"\\nüéØ Best Parameters Found:\")\n",
    "    print(best_params)\n",
    "    print(f\"üèÜ Best Validation Accuracy: {best_acc:.4f}\")\n",
    "    return best_params, best_acc\n",
    "\n",
    "\n",
    "# Run random search\n",
    "best_params, best_acc = random_search(n_trials=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e6bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f1ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Final Training with Best Hyperparameters\n",
    "# ===========================\n",
    "\n",
    "# Unpack best params from random search\n",
    "lr = best_params[\"lr\"]\n",
    "dropout = best_params[\"dropout\"]\n",
    "weight_decay = best_params[\"weight_decay\"]\n",
    "\n",
    "print(\"\\nüöÄ Training final model with best hyperparameters:\")\n",
    "print(f\"lr={lr:.5f}, dropout={dropout:.3f}, weight_decay={weight_decay:.6f}\")\n",
    "\n",
    "# Use full training subset now\n",
    "full_loader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "# Build model with best hyperparameters\n",
    "final_model = DeepCNN(num_classes=7, dropout_prob=dropout)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "final_model.to(device)\n",
    "\n",
    "# Compute class weights on full training set\n",
    "train_labels = [train_dataset.targets[i] for i in train_subset.indices]\n",
    "classes = np.unique(train_labels)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=train_labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "# Train final model\n",
    "final_model, final_history = train_model_with_history(\n",
    "    final_model, full_loader, val_loader,\n",
    "    criterion, optimizer, scheduler,\n",
    "    num_epochs=20,  # more epochs for better training now\n",
    "    device=device,\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "# Best validation accuracy\n",
    "final_val_acc = max(final_history[\"val_acc\"])\n",
    "print(f\"\\nüèÜ Final Model Validation Accuracy: {final_val_acc:.4f}\")\n",
    "\n",
    "# Optionally save model\n",
    "torch.save(final_model.state_dict(), \"best_deepcnn_model.pth\")\n",
    "print(\"üíæ Final model saved as best_deepcnn_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
