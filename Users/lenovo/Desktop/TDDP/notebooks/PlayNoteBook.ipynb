{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fbe46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5613e7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Full optimization + hyperparameter tuning pipeline using Optuna for your CNN.\n",
    "- Make sure configs.MULTIVIEW_TRAIN_DIR and configs.MULTIVIEW_TEST_DIR are set.\n",
    "- Requires: torch, torchvision, optuna, tqdm, sklearn, matplotlib, seaborn, numpy\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, balanced_accuracy_score, f1_score, precision_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from optuna.exceptions import TrialPruned\n",
    "\n",
    "# Add your utils/configs path if needed\n",
    "sys.path.append('../Utils')\n",
    "import configs\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "065699ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0ca32b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# MixUp helper\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha <= 0:\n",
    "        return x, y, None, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, preds, y_a, y_b, lam):\n",
    "    return lam * criterion(preds, y_a) + (1 - lam) * criterion(preds, y_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "247ffe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TunableDeepCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7, base_filters=32, dropout_p=0.5, use_dropout2d=True):\n",
    "        super(TunableDeepCNN, self).__init__()\n",
    "        f = base_filters\n",
    "        # Block 1\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, f, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(f),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(f, f, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(f),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        # Block 2\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(f, f*2, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(f*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(f*2, f*2, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(f*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        # Block 3\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(f*2, f*4, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(f*4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(f*4, f*4, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(f*4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling -> produces f*4 features\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully connected\n",
    "        self.fc1 = nn.Linear(f*4, max(128, f*4))\n",
    "        self.fc2 = nn.Linear(max(128, f*4), num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.use_dropout2d = use_dropout2d\n",
    "        self.dropout2d = nn.Dropout2d(0.3) if use_dropout2d else nn.Identity()\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.dropout2d(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.dropout2d(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.gap(x)               # [B, C, 1, 1]\n",
    "        x = x.view(x.size(0), -1)     # [B, C]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e70632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard transforms (we will create train/test transforms inside objective for augmentation choices)\n",
    "base_transform_train = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=12),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "base_transform_test = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0109e46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Ash', 'Beech', 'Douglas Fir', 'Oak', 'Pine', 'Red Oak', 'Spruce']\n",
      "NUM_CLASSES: 7\n"
     ]
    }
   ],
   "source": [
    "# load full datasets once (transforms will be replaced by dataset.transform later)\n",
    "full_train_dataset = datasets.ImageFolder(configs.MULTIVIEW_TRAIN_DIR, transform=base_transform_train)\n",
    "full_test_dataset = datasets.ImageFolder(configs.MULTIVIEW_TEST_DIR, transform=base_transform_test)\n",
    "\n",
    "NUM_CLASSES = len(full_train_dataset.classes)\n",
    "print(\"Classes:\", full_train_dataset.classes)\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac986d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, use_mixup=False, mixup_alpha=0.4):\n",
    "    model.train()\n",
    "    running_loss, running_corrects, total = 0, 0, 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if use_mixup:\n",
    "            inputs, y_a, y_b, lam = mixup_data(inputs, labels, alpha=mixup_alpha)\n",
    "            outputs = model(inputs)\n",
    "            loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        if not use_mixup:\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        total += inputs.size(0)\n",
    "\n",
    "    return running_loss/total, (running_corrects.double()/total if not use_mixup else None)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, running_corrects, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()*inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total += inputs.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return running_loss/total, running_corrects.double()/total, all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41dcaee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    set_seed(42)\n",
    "\n",
    "    # Hyperparameters\n",
    "    base_filters = trial.suggest_categorical(\"base_filters\", [16,32,48])\n",
    "    dropout_p = trial.suggest_uniform(\"dropout_p\", 0.2,0.6)\n",
    "    use_dropout2d = trial.suggest_categorical(\"use_dropout2d\",[True,False])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\",[\"adam\",\"sgd\"])\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2) if optimizer_name==\"adam\" else trial.suggest_loguniform(\"lr\", 1e-4, 1e-1)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-7, 1e-3)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\",[16,32,48])\n",
    "    mixup = trial.suggest_categorical(\"mixup\",[False,True])\n",
    "    mixup_alpha = trial.suggest_uniform(\"mixup_alpha\",0.1,0.6) if mixup else 0\n",
    "    label_smoothing = trial.suggest_uniform(\"label_smoothing\",0.0,0.2)\n",
    "\n",
    "    max_epochs, patience = 30, 8\n",
    "\n",
    "    # Train/val split\n",
    "    num_train = len(full_train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    random.shuffle(indices)\n",
    "    split = int(0.2*num_train)\n",
    "    val_idx, train_idx = indices[:split], indices[split:]\n",
    "\n",
    "    train_subset, val_subset = Subset(full_train_dataset, train_idx), Subset(full_train_dataset, val_idx)\n",
    "    train_labels = [full_train_dataset.targets[i] for i in train_idx]\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model & optimizer\n",
    "    model = TunableDeepCNN(NUM_CLASSES, base_filters, dropout_p, use_dropout2d).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=label_smoothing)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) if optimizer_name==\"adam\" else \\\n",
    "                optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_acc, patience_counter, best_model_wts = 0,0,copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        train_loss, _ = train_one_epoch(model, train_loader, criterion, optimizer, device, mixup, mixup_alpha)\n",
    "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        trial.report(val_acc, epoch)\n",
    "        if trial.should_prune(): raise TrialPruned()\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc, best_model_wts, patience_counter = val_acc, copy.deepcopy(model.state_dict()), 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience: break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return float(best_val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0494a88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-09 21:47:21,460] A new study created in memory with name: no-name-db136655-f140-41a2-b8e3-dd4056c3afbe\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def run_study(n_trials=20):\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Best trial value:\", study.best_trial.value)\n",
    "    print(\"Best params:\", study.best_trial.params)\n",
    "    return study\n",
    "\n",
    "study = run_study(10)  # try 10 first, then increase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f16eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_best(study):\n",
    "    best_params = study.best_trial.params\n",
    "    model = TunableDeepCNN(NUM_CLASSES, best_params[\"base_filters\"],\n",
    "                           best_params[\"dropout_p\"], best_params[\"use_dropout2d\"]).to(device)\n",
    "\n",
    "    # reload best state dict if saved, else retrain\n",
    "    print(\"Evaluating with best params:\", best_params)\n",
    "\n",
    "    test_loader = DataLoader(full_test_dataset, batch_size=best_params.get(\"batch_size\",32), shuffle=False)\n",
    "    all_preds, all_labels = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs,1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "    print(\"Report:\\n\", classification_report(all_labels, all_preds))\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.show()\n",
    "\n",
    "evaluate_best(study)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
