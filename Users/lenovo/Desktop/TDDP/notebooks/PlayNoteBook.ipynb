{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fbe46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5613e7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs paths:\n",
      " TRAIN: C:\\Users\\lenovo\\Desktop\\TDDP\\outputs\\MultiViewData\\train\n",
      " TEST : C:\\Users\\lenovo\\Desktop\\TDDP\\outputs\\MultiViewData\\test\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# %% Imports & Setup\n",
    "import os, sys, time, random, copy, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import optuna\n",
    "from optuna.exceptions import TrialPruned\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Add your utils/configs path if needed\n",
    "sys.path.append('../Utils')\n",
    "import configs  # must provide MULTIVIEW_TRAIN_DIR and MULTIVIEW_TEST_DIR (ImageFolder structure)\n",
    "\n",
    "print(\"Configs paths:\")\n",
    "print(\" TRAIN:\", configs.MULTIVIEW_TRAIN_DIR)\n",
    "print(\" TEST :\", configs.MULTIVIEW_TEST_DIR)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# reproducible seed\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "247ffe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Model definition (TunableDeepCNN)\n",
    "class TunableDeepCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7, base_filters=32, dropout_p=0.5, use_dropout2d=True):\n",
    "        super(TunableDeepCNN, self).__init__()\n",
    "        f = base_filters\n",
    "        # Block 1\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, f, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(f),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(f, f, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(f),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        # Block 2\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(f, f*2, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(f*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(f*2, f*2, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(f*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        # Block 3\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(f*2, f*4, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(f*4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(f*4, f*4, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(f*4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling -> produces f*4 features\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully connected\n",
    "        self.fc1 = nn.Linear(f*4, max(128, f*4))\n",
    "        self.fc2 = nn.Linear(max(128, f*4), num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.use_dropout2d = use_dropout2d\n",
    "        self.dropout2d = nn.Dropout2d(0.3) if use_dropout2d else nn.Identity()\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.dropout2d(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.dropout2d(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.gap(x)               # [B, C, 1, 1]\n",
    "        x = x.view(x.size(0), -1)     # [B, C]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e70632e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Ash', 'Beech', 'Douglas Fir', 'Oak', 'Pine', 'Red Oak', 'Spruce']\n",
      "NUM_CLASSES: 7\n"
     ]
    }
   ],
   "source": [
    "# %% Data transforms and splits\n",
    "base_transform_train = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((128, 128)),   # smaller for speed â€” change to 224 if you have memory\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=12),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "base_transform_test = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load full datasets\n",
    "full_train_dataset = datasets.ImageFolder(configs.MULTIVIEW_TRAIN_DIR, transform=base_transform_train)\n",
    "full_test_dataset  = datasets.ImageFolder(configs.MULTIVIEW_TEST_DIR, transform=base_transform_test)\n",
    "\n",
    "NUM_CLASSES = len(full_train_dataset.classes)\n",
    "print(\"Classes:\", full_train_dataset.classes)\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
    "\n",
    "# Create train/validation split (80/20) for Optuna and final training\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# We'll use separate transforms for validation (no augmentation)\n",
    "# update val_dataset to use test transform (non-augmented)\n",
    "# random_split returns Subset objects; override dataset attribute transform via a wrapper\n",
    "class SubsetWithTransform(Subset):\n",
    "    def __init__(self, subset, transform):\n",
    "        super().__init__(subset.dataset, subset.indices)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.indices[idx]\n",
    "        path, target = self.dataset.samples[real_idx]\n",
    "        img = self.dataset.loader(path)\n",
    "        img = self.transform(img)\n",
    "        return img, target\n",
    "\n",
    "val_dataset = SubsetWithTransform(train_dataset, base_transform_test)\n",
    "\n",
    "# DataLoaders (default batch size used by Optuna objective will override if needed)\n",
    "DEFAULT_BS = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=DEFAULT_BS, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=DEFAULT_BS, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(full_test_dataset, batch_size=DEFAULT_BS, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0109e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% training/validation helpers (return history)\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, use_mixup=False, mixup_alpha=0.4):\n",
    "    model.train()\n",
    "    running_loss, running_corrects, total = 0.0, 0, 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        total += inputs.size(0)\n",
    "    return running_loss / total, running_corrects.double() / total\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, running_corrects, total = 0.0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total += inputs.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return running_loss / total, running_corrects.double() / total, all_labels, all_preds\n",
    "\n",
    "def train_model_with_history(model, train_loader, val_loader, criterion, optimizer, scheduler=None,\n",
    "                             num_epochs=8, device=device, patience=3):\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    best_val_acc = 0.0\n",
    "    best_weights = copy.deepcopy(model.state_dict())\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            # scheduler can be ReduceLROnPlateau or other; if plateau requires val_loss, call step accordingly\n",
    "            try:\n",
    "                scheduler.step(val_loss)\n",
    "            except TypeError:\n",
    "                # call without args\n",
    "                scheduler.step()\n",
    "\n",
    "        history[\"train_loss\"].append(float(tr_loss))\n",
    "        history[\"train_acc\"].append(float(tr_acc))\n",
    "        history[\"val_loss\"].append(float(val_loss))\n",
    "        history[\"val_acc\"].append(float(val_acc))\n",
    "\n",
    "        # early save best\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = float(val_acc)\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac986d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-11 12:47:03,686] A new study created in memory with name: no-name-577778c5-db16-4754-88e8-1edd60fdc710\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4e862483294fd4923c53e9f207b668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% Optuna objective (fast; uses subset of train set)\n",
    "def objective(trial):\n",
    "    # hyperparams\n",
    "    base_filters = trial.suggest_categorical(\"base_filters\", [16, 32, 48])\n",
    "    dropout_p = trial.suggest_uniform(\"dropout_p\", 0.2, 0.6)\n",
    "    use_dropout2d = trial.suggest_categorical(\"use_dropout2d\", [True, False])\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-7, 1e-3)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32])\n",
    "\n",
    "    # Create a small subset of training data for fast trials\n",
    "    small_frac = 0.25\n",
    "    n_small = max(32, int(small_frac * len(train_dataset)))\n",
    "    small_indices = np.random.choice(range(len(train_dataset)), size=n_small, replace=False)\n",
    "    small_subset = Subset(train_dataset, small_indices)\n",
    "    small_loader = DataLoader(small_subset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # validation loader: use val_loader defined earlier but maybe smaller batch\n",
    "    val_loader_small = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # model + criterion + optimizer\n",
    "    model = TunableDeepCNN(num_classes=NUM_CLASSES, base_filters=base_filters, dropout_p=dropout_p, use_dropout2d=use_dropout2d).to(device)\n",
    "\n",
    "    # compute class weights on small subset (robust to imbalance)\n",
    "    labels_small = []\n",
    "    for _, lab in small_subset:\n",
    "        labels_small.append(int(lab))\n",
    "    classes = np.unique(labels_small)\n",
    "    if len(classes) == 0:\n",
    "        class_weights = torch.ones(NUM_CLASSES, device=device)\n",
    "    else:\n",
    "        cw = compute_class_weight('balanced', classes=np.unique(labels_small), y=labels_small)\n",
    "        class_weights = torch.tensor(cw, dtype=torch.float, device=device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "\n",
    "    # train short with small patience and report intermediate results for pruning\n",
    "    best_val = 0.0\n",
    "    for epoch in range(1, 9):  # short: 8 epochs\n",
    "        model.train()\n",
    "        for X, y in small_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # evaluate on validation (full val_dataset but small batch)\n",
    "        model.eval()\n",
    "        val_loss, val_acc, _, _ = validate(model, val_loader_small, criterion, device)\n",
    "        trial.report(float(val_acc), epoch)\n",
    "\n",
    "        # pruning check\n",
    "        if trial.should_prune():\n",
    "            raise TrialPruned()\n",
    "\n",
    "        if val_acc > best_val:\n",
    "            best_val = float(val_acc)\n",
    "\n",
    "    return best_val\n",
    "\n",
    "# Run a quick study first\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=6, timeout=None, show_progress_bar=True)\n",
    "print(\"Fast study best:\", study.best_trial.params, \"val_acc=\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dcaee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-11 12:39:41,660] A new study created in memory with name: no-name-bba37c50-e95e-41f7-9593-7748ca4f93f5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbe7561a48e435e88701835feb1541b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-11 12:39:41,689] Trial 0 failed with parameters: {'lr': 0.000725780656152857, 'dropout': 0.37567359902864556, 'weight_decay': 0.00017679551481686453} because of the following error: NameError(\"name 'train_subset' is not defined\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_72140\\3638115026.py\", line 15, in objective\n",
      "    subset_size = int(0.3 * len(train_subset))\n",
      "                                ^^^^^^^^^^^^\n",
      "NameError: name 'train_subset' is not defined\n",
      "[W 2025-09-11 12:39:41,691] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Run Optuna study\u001b[39;00m\n\u001b[0;32m     47\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# fewer trials for faster search\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest validation accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_value)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\study.py:489\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    389\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 64\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    249\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    252\u001b[0m ):\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     12\u001b[0m weight_decay \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1e-5\u001b[39m, \u001b[38;5;241m1e-3\u001b[39m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Subsample training data for speed (30% of original)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m subset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.3\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_subset\u001b[49m))\n\u001b[0;32m     16\u001b[0m subset_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(train_subset\u001b[38;5;241m.\u001b[39mindices, subset_size, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m trial_subset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mSubset(train_dataset, subset_indices)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_subset' is not defined"
     ]
    }
   ],
   "source": [
    "# %% Retrain final best model on full train dataset (longer) then evaluate on test set\n",
    "best_params = study.best_trial.params\n",
    "print(\"Best params to retrain:\", best_params)\n",
    "\n",
    "# create dataloaders for full training with chosen batch size\n",
    "final_bs = int(best_params.get(\"batch_size\", 32))\n",
    "full_train_loader = DataLoader(train_dataset, batch_size=final_bs, shuffle=True, num_workers=4, pin_memory=True)\n",
    "final_val_loader = DataLoader(val_dataset, batch_size=final_bs, shuffle=False, num_workers=4, pin_memory=True)\n",
    "final_test_loader = DataLoader(full_test_dataset, batch_size=final_bs, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# build model with best params\n",
    "model_final = TunableDeepCNN(num_classes=NUM_CLASSES,\n",
    "                            base_filters=int(best_params.get(\"base_filters\", 32)),\n",
    "                            dropout_p=float(best_params.get(\"dropout_p\", 0.5)),\n",
    "                            use_dropout2d=bool(best_params.get(\"use_dropout2d\", True))).to(device)\n",
    "\n",
    "# class weights computed on entire train_dataset\n",
    "train_labels = [y for _, y in train_dataset]\n",
    "if len(train_labels) == 0:\n",
    "    cw = np.ones(NUM_CLASSES)\n",
    "else:\n",
    "    cw = compute_class_weight('balanced', classes=np.arange(NUM_CLASSES), y=train_labels)\n",
    "class_weights = torch.tensor(cw, dtype=torch.float, device=device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model_final.parameters(), lr=float(best_params.get(\"lr\", 1e-4)), weight_decay=float(best_params.get(\"weight_decay\", 1e-6)))\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=4, factor=0.5)\n",
    "\n",
    "# Train for more epochs (final)\n",
    "final_epochs = 30\n",
    "model_final, history_final = train_model_with_history(model_final, full_train_loader, final_val_loader,\n",
    "                                                      criterion, optimizer, scheduler,\n",
    "                                                      num_epochs=final_epochs, device=device, patience=6)\n",
    "\n",
    "# Save best final model\n",
    "out_path = Path(\"best_final_model.pth\")\n",
    "torch.save(model_final.state_dict(), out_path)\n",
    "print(\"Saved final model to:\", out_path)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_labels, test_preds = validate(model_final, final_test_loader, criterion, device)\n",
    "print(f\"Test Acc: {test_acc:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(test_labels, test_preds, target_names=full_train_dataset.classes))\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=full_train_dataset.classes, yticklabels=full_train_dataset.classes, cmap=\"Blues\")\n",
    "plt.ylabel(\"True\"); plt.xlabel(\"Predicted\"); plt.title(\"Final Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history_final[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(history_final[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend(); plt.title(\"Loss\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history_final[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(history_final[\"val_acc\"], label=\"val_acc\")\n",
    "plt.legend(); plt.title(\"Accuracy\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
