{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5368146e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "âš ï¸ Running on cpu (CUDA not available)\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Cell 1: Imports & Config\n",
    "# =======================\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score,\n",
    "    f1_score, precision_score, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import open3d as o3d  # âœ… NEW: Open3D for point cloud loading\n",
    "\n",
    "# Import custom model & configs\n",
    "import sys\n",
    "sys.path.append(\"../Utils\")\n",
    "from models import DGCNN  # You will have models.py with DGCNN implemented\n",
    "import configs\n",
    "\n",
    "# Paths\n",
    "TRAIN_DIR = Path(configs.TRAIN_DIR)\n",
    "TEST_DIR = Path(configs.TEST_DIR)\n",
    "MODEL_DIR = Path(configs.MODEL_DIR)\n",
    "\n",
    "\n",
    "# Force CPU device (no CUDA for now)\n",
    "class CFG:\n",
    "    num_points = 1024\n",
    "    batch_size = 16\n",
    "    epochs = 20\n",
    "    lr = 1e-3\n",
    "    device = torch.device(\"cpu\")  # âœ… FORCE CPU\n",
    "    k = 20\n",
    "    emb_dims = 1024\n",
    "    dropout = 0.5\n",
    "    num_classes = len(os.listdir(TRAIN_DIR))\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"âš ï¸ Running on {CFG.device} (CUDA {'available' if torch.cuda.is_available() else 'not available'})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44843b2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chardet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Cell 2: Point Cloud Loader (Robust)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopen3d\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mo3d\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_point_file\u001b[39m(path):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chardet'"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Cell 2: Robust Point Cloud Loader\n",
    "# ==========================\n",
    "import chardet\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "def read_point_file(path):\n",
    "    \"\"\"\n",
    "    Universal point cloud reader.\n",
    "    Reads any .txt/.xyz/.pts file regardless of encoding.\n",
    "    Returns: numpy array (N,3) or (N,6) if RGB is present.\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        raw = f.read()\n",
    "        enc = chardet.detect(raw)[\"encoding\"] or \"latin1\"\n",
    "        text = raw.decode(enc, errors=\"ignore\")\n",
    "\n",
    "    data = []\n",
    "    for line in text.strip().splitlines():\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        try:\n",
    "            x, y, z = map(float, parts[:3])\n",
    "            if len(parts) >= 6:\n",
    "                r, g, b = map(float, parts[3:6])\n",
    "                data.append([x, y, z, r / 255.0, g / 255.0, b / 255.0])\n",
    "            else:\n",
    "                data.append([x, y, z])\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    points = np.array(data, dtype=np.float32)\n",
    "    return points\n",
    "\n",
    "\n",
    "def farthest_point_sampling(points, n_samples=1024):\n",
    "    \"\"\"\n",
    "    Farthest Point Sampling (FPS) using Open3D.\n",
    "    Works for XYZ or XYZ+RGB.\n",
    "    \"\"\"\n",
    "    xyz = points[:, :3]\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(xyz)\n",
    "    pcd = pcd.farthest_point_down_sample(n_samples)\n",
    "    sampled_xyz = np.asarray(pcd.points, dtype=np.float32)\n",
    "\n",
    "    # Preserve RGB if present\n",
    "    if points.shape[1] > 3:\n",
    "        sampled_rgb = []\n",
    "        for sp in sampled_xyz:\n",
    "            idx = np.linalg.norm(xyz - sp, axis=1).argmin()\n",
    "            sampled_rgb.append(points[idx, 3:6])\n",
    "        sampled_rgb = np.array(sampled_rgb, dtype=np.float32)\n",
    "        return np.hstack((sampled_xyz, sampled_rgb))\n",
    "\n",
    "    return sampled_xyz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12851280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Cell 3: Custom Dataset\n",
    "# =======================\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, root_dir, n_points=1024, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.classes = sorted([d.name for d in self.root_dir.iterdir() if d.is_dir()])\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        self.n_points = n_points\n",
    "        self.transform = transform\n",
    "\n",
    "        # Collect file paths\n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            for f in (self.root_dir / cls).glob(\"*.*\"):\n",
    "                self.files.append(f)\n",
    "                self.labels.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        points = read_point_file(file)\n",
    "        # Downsample using FPS to n_points\n",
    "        if len(points) > self.n_points:\n",
    "            points = farthest_point_sampling(points, n_samples=self.n_points)\n",
    "        else:\n",
    "            # Pad if not enough points\n",
    "            pad_size = self.n_points - len(points)\n",
    "            points = np.vstack([points, np.zeros((pad_size, points.shape[1]), dtype=np.float32)])\n",
    "\n",
    "        if self.transform:\n",
    "            points = self.transform(points)\n",
    "\n",
    "        return torch.tensor(points.T, dtype=torch.float32), label  # (C, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f34ff7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 557 training samples, 134 test samples\n",
      "Classes: ['Ash', 'Beech', 'Douglas Fir', 'Oak', 'Pine', 'Red Oak', 'Spruce']\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Cell 4: Dataloaders\n",
    "# =======================\n",
    "train_dataset = PointCloudDataset(TRAIN_DIR, n_points=CFG.num_points)\n",
    "test_dataset = PointCloudDataset(TEST_DIR, n_points=CFG.num_points)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"âœ… Loaded {len(train_dataset)} training samples, {len(test_dataset)} test samples\")\n",
    "print(f\"Classes: {train_dataset.classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13d22c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DGCNN model initialized with 7 classes\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Cell 5: Model Setup\n",
    "# =======================\n",
    "args = type('', (), {})()  # empty object for args\n",
    "args.k = CFG.k\n",
    "args.emb_dims = CFG.emb_dims\n",
    "args.dropout = CFG.dropout\n",
    "\n",
    "model = DGCNN(args, output_channels=CFG.num_classes).to(CFG.device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG.lr)\n",
    "\n",
    "print(f\"âœ… DGCNN model initialized with {CFG.num_classes} classes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "801e421a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D WARNING] Read PTS failed with exception: vector too long\n",
      "[Open3D WARNING] Read geometry::PointCloud failed: unknown file extension for C:\\Users\\lenovo\\Desktop\\TDDP\\dataset\\train\\Spruce\\65_88.txt (format: auto).\n",
      "[Open3D WARNING] Read geometry::PointCloud failed: unknown file extension for C:\\Users\\lenovo\\Desktop\\TDDP\\dataset\\train\\Douglas Fir\\61_45.txt (format: auto).\n",
      "[Open3D WARNING] Read geometry::PointCloud failed: unknown file extension for C:\\Users\\lenovo\\Desktop\\TDDP\\dataset\\train\\Douglas Fir\\61_20.txt (format: auto).\n",
      "[Open3D WARNING] Read geometry::PointCloud failed: unknown file extension for C:\\Users\\lenovo\\Desktop\\TDDP\\dataset\\train\\Spruce\\35_23.txt (format: auto).\n",
      "[Open3D WARNING] Read geometry::PointCloud failed: unknown file extension for C:\\Users\\lenovo\\Desktop\\TDDP\\dataset\\train\\Douglas Fir\\31_59.txt (format: auto).\n",
      "[Open3D WARNING] Read geometry::PointCloud failed: unknown file extension for C:\\Users\\lenovo\\Desktop\\TDDP\\dataset\\train\\Douglas Fir\\31_54.txt (format: auto).\n",
      "[Open3D WARNING] Read geometry::PointCloud failed: unknown file extension for C:\\Users\\lenovo\\Desktop\\TDDP\\dataset\\train\\Spruce\\34_46.txt (format: auto).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "âŒ Error reading C:\\Users\\lenovo\\Desktop\\TDDP\\dataset\\train\\Red Oak\\Baum7_256 brb_pssuâ• Ãªd- Cloud.txt: 'utf-8' codec can't decode byte 0xa6 in position 147: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m, in \u001b[0;36mread_point_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     pcd \u001b[38;5;241m=\u001b[39m \u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_point_cloud\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(pcd\u001b[38;5;241m.\u001b[39mpoints, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa6 in position 147: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m best_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(CFG\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m---> 39\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     test_acc, _, _ \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCFG\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Test Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, criterion, optimizer)\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      6\u001b[0m total_loss, correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[13], line 26\u001b[0m, in \u001b[0;36mPointCloudDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     23\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles[idx]\n\u001b[0;32m     24\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m---> 26\u001b[0m points \u001b[38;5;241m=\u001b[39m \u001b[43mread_point_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Downsample using FPS to n_points\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(points) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_points:\n",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m, in \u001b[0;36mread_point_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m points  \u001b[38;5;66;03m# (N,3)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ Error reading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: âŒ Error reading C:\\Users\\lenovo\\Desktop\\TDDP\\dataset\\train\\Red Oak\\Baum7_256 brb_pssuâ• Ãªd- Cloud.txt: 'utf-8' codec can't decode byte 0xa6 in position 147: invalid start byte"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Cell 6: Training Loop\n",
    "# =======================\n",
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for points, labels in loader:\n",
    "        points, labels = points.to(CFG.device), torch.tensor(labels).to(CFG.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(points)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds_list, labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for points, labels in loader:\n",
    "            points = points.to(CFG.device)\n",
    "            outputs = model(points)\n",
    "            preds_list.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            labels_list.extend(labels)\n",
    "\n",
    "    acc = accuracy_score(labels_list, preds_list)\n",
    "    return acc, preds_list, labels_list\n",
    "\n",
    "\n",
    "best_acc = 0\n",
    "for epoch in range(CFG.epochs):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "    test_acc, _, _ = evaluate(model, test_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{CFG.epochs} | Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), MODEL_DIR / \"best_dgcnn.pth\")\n",
    "        print(f\"ðŸ’¾ Model saved with acc {best_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed39de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Cell 7: Evaluation\n",
    "# =======================\n",
    "best_model = DGCNN(args, output_channels=CFG.num_classes).to(CFG.device)\n",
    "best_model.load_state_dict(torch.load(MODEL_DIR / \"best_dgcnn.pth\"))\n",
    "\n",
    "acc, preds, labels = evaluate(best_model, test_loader)\n",
    "print(f\"âœ… Final Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(labels, preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=train_dataset.classes, yticklabels=train_dataset.classes, cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
