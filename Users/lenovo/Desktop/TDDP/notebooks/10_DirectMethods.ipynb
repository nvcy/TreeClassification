{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50043bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2048c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Config\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score,\n",
    "    f1_score, precision_score, confusion_matrix, classification_report\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../Utils\")\n",
    "from models  import DGCNN  #  DGCNN & PointNet implementation\n",
    "import configs\n",
    "# Paths\n",
    "\n",
    "TRAIN_DIR = configs.TRAIN_DIR\n",
    "TEST_DIR = configs.TEST_DIR\n",
    "MODEL_DIR = configs.MODEL_DIR\n",
    "\n",
    "\n",
    "# Config class\n",
    "class CFG:\n",
    "    num_points = 1024\n",
    "    batch_size = 16\n",
    "    epochs = 20\n",
    "    lr = 1e-3\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    k = 20\n",
    "    emb_dims = 1024\n",
    "    dropout = 0.5\n",
    "    num_classes = len(os.listdir(TRAIN_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4647b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify and setup environment\n",
    "import random\n",
    "\n",
    "# 1. Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# 2. Verify paths\n",
    "for path_name, path in [(\"TRAIN_DIR\", TRAIN_DIR), (\"TEST_DIR\", TEST_DIR), (\"MODEL_DIR\", MODEL_DIR)]:\n",
    "    path = Path(path)\n",
    "    if not path.exists() and path_name != \"MODEL_DIR\":\n",
    "        raise ValueError(f\"{path_name} does not exist: {path}\")\n",
    "    elif path_name == \"MODEL_DIR\":\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "    print(f\"‚úì {path_name}: {path}\")\n",
    "\n",
    "# 3. GPU setup if available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"‚úì Using GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Using CPU - training might be slow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e465f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Dataset + Utils\n",
    "def load_point_file_safe(file_path):\n",
    "    \"\"\"Load a point cloud file with flexible delimiters and varying columns.\"\"\"\n",
    "    for delim in [None, \" \", \",\", \"\\t\"]:\n",
    "        try:\n",
    "            pc = np.loadtxt(file_path, delimiter=delim)\n",
    "            if pc.ndim == 1:  # single point\n",
    "                pc = pc[np.newaxis, :]\n",
    "            return pc\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise ValueError(f\"Could not read {file_path}\")\n",
    "\n",
    "def normalize_unit(pc):\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    return pc / m\n",
    "\n",
    "def farthest_point_sampling(pc, n_points):\n",
    "    \"\"\"Sample N farthest points from a point cloud.\"\"\"\n",
    "    N, _ = pc.shape\n",
    "    if N <= n_points:\n",
    "        return np.pad(pc, ((0, n_points-N), (0, 0)))\n",
    "    centroids = np.zeros((n_points,))\n",
    "    distance = np.ones(N) * 1e10\n",
    "    farthest = np.random.randint(0, N)\n",
    "    for i in range(n_points):\n",
    "        centroids[i] = farthest\n",
    "        dist = np.sum((pc - pc[farthest])**2, axis=1)\n",
    "        distance = np.minimum(distance, dist)\n",
    "        farthest = np.argmax(distance)\n",
    "    return pc[centroids.astype(np.int32)]\n",
    "\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, root_dir, num_points=1024):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.num_points = num_points\n",
    "        self.files, self.labels = [], []\n",
    "        for i, class_dir in enumerate(sorted(self.root_dir.iterdir())):\n",
    "            if class_dir.is_dir():\n",
    "                for f in class_dir.glob(\"*.*\"):\n",
    "                    self.files.append(f)\n",
    "                    self.labels.append(i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            label = int(self.labels[idx])\n",
    "            pc = load_point_file_safe(str(self.files[idx]))\n",
    "\n",
    "            # ‚úÖ Keep only XYZ (first 3 columns)\n",
    "            if pc.shape[1] > 3:\n",
    "                pc = pc[:, :3]\n",
    "\n",
    "            pc = normalize_unit(pc)\n",
    "            pc = farthest_point_sampling(pc, self.num_points)\n",
    "            return torch.from_numpy(pc.T.astype(np.float32)), label  # [3, N]\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"‚ö†Ô∏è Dataset error idx={idx}: {e}\")\n",
    "            return torch.zeros((3, self.num_points), dtype=torch.float32), 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be96c657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 557, Test samples: 134\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Dataloaders\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Initialize datasets\n",
    "train_dataset = PointCloudDataset(TRAIN_DIR, num_points=CFG.num_points)\n",
    "test_dataset = PointCloudDataset(TEST_DIR, num_points=CFG.num_points)\n",
    "\n",
    "# Split training data into train and validation\n",
    "val_size = int(0.1 * len(train_dataset))  # 10% for validation\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_subset, val_subset = random_split(\n",
    "    train_dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create data loaders with num_workers based on CPU count\n",
    "num_workers = min(4, os.cpu_count() or 1)\n",
    "train_loader = DataLoader(\n",
    "    train_subset, \n",
    "    batch_size=CFG.batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_subset, \n",
    "    batch_size=CFG.batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=CFG.batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Training:   {len(train_subset):5d} samples\")\n",
    "print(f\"Validation: {len(val_subset):5d} samples\")\n",
    "print(f\"Test:       {len(test_dataset):5d} samples\")\n",
    "print(f\"Using {num_workers} workers for data loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd5cfee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model setup\n",
    "args = CFG\n",
    "model = DGCNN(args, output_channels=CFG.num_classes).to(CFG.device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a34959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify setup\n",
    "print(f\"Device being used: {CFG.device}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "print(f\"Training samples per batch: {CFG.batch_size}\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")\n",
    "print(f\"Total training samples: {len(train_dataset)}\")\n",
    "print(f\"Total test samples: {len(test_dataset)}\")\n",
    "print(f\"Number of classes: {CFG.num_classes}\")\n",
    "\n",
    "# Try a forward pass with a small batch\n",
    "for batch in train_loader:\n",
    "    points, labels = batch\n",
    "    points = points.to(CFG.device)\n",
    "    labels = labels.to(CFG.device)\n",
    "    try:\n",
    "        output = model(points)\n",
    "        print(f\"Forward pass successful. Output shape: {output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Forward pass failed: {e}\")\n",
    "    break  # Only test one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70df8686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training & Evaluation functions\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    for points, labels in loader:\n",
    "        points, labels = points.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(points)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * points.size(0)\n",
    "        total_correct += preds.argmax(1).eq(labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "    return total_loss / total_samples, total_correct / total_samples\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for points, labels in loader:\n",
    "            points, labels = points.to(device), labels.to(device)\n",
    "            preds = model(points)\n",
    "            loss = criterion(preds, labels)\n",
    "            total_loss += loss.item() * points.size(0)\n",
    "            total_correct += preds.argmax(1).eq(labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.argmax(1).cpu().numpy())\n",
    "    return (total_loss / total_samples,\n",
    "            total_correct / total_samples,\n",
    "            np.array(all_labels),\n",
    "            np.array(all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "357d7a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error during training: Torch not compiled with CUDA enabled\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(CFG\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m         tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n\u001b[0;32m     16\u001b[0m         val_loss, val_acc, val_labels, val_preds \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, criterion, CFG\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[36], line 8\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m      6\u001b[0m points, labels \u001b[38;5;241m=\u001b[39m points\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 8\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(preds, labels)\n\u001b[0;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\Desktop\\TDDP\\notebooks\\../Utils\\models.py:127\u001b[0m, in \u001b[0;36mDGCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    126\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 127\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mget_graph_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m    129\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\Desktop\\TDDP\\notebooks\\../Utils\\models.py:40\u001b[0m, in \u001b[0;36mget_graph_feature\u001b[1;34m(x, k, idx)\u001b[0m\n\u001b[0;32m     37\u001b[0m     idx \u001b[38;5;241m=\u001b[39m knn(x, k\u001b[38;5;241m=\u001b[39mk)   \u001b[38;5;66;03m# (batch_size, num_points, k)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m idx_base \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mnum_points\n\u001b[0;32m     42\u001b[0m idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m+\u001b[39m idx_base\n\u001b[0;32m     44\u001b[0m idx \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:363\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m     )\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    367\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Cell 6: Training Loop\n",
    "# Initialize tracking variables\n",
    "best_val_acc = 0\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "start_time = time.time()\n",
    "patience = 5  # early stopping patience\n",
    "patience_counter = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(CFG.epochs):\n",
    "        # Training phase\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, CFG.device)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc, val_labels, val_preds = evaluate(model, val_loader, criterion, CFG.device)\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(tr_loss)\n",
    "        train_accs.append(tr_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss\n",
    "            }\n",
    "            torch.save(checkpoint, str(MODEL_DIR / \"best_model.pth\"))\n",
    "            print(f\"‚úì Saved best model with validation accuracy: {val_acc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{CFG.epochs} | \"\n",
    "              f\"Train Loss: {tr_loss:.4f}, Train Acc: {tr_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"‚ö†Ô∏è Early stopping triggered after {patience} epochs without improvement\")\n",
    "            break\n",
    "\n",
    "    # Training completion\n",
    "    exec_time = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Training completed in {exec_time:.2f} seconds\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    # Final test evaluation\n",
    "    test_loss, test_acc, test_labels, test_preds = evaluate(model, test_loader, criterion, CFG.device)\n",
    "    print(f\"Final test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during training: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631fe505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Metrics\n",
    "overall_acc = accuracy_score(val_labels, val_preds)\n",
    "balanced_acc = balanced_accuracy_score(val_labels, val_preds)\n",
    "f1 = f1_score(val_labels, val_preds, average=\"weighted\")\n",
    "prec_per_class = precision_score(val_labels, val_preds, average=None)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"üìä Metrics:\")\n",
    "print(f\"Overall Accuracy: {overall_acc:.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision per class: {prec_per_class}\")\n",
    "print(f\"Execution Time (s): {exec_time:.2f}\")\n",
    "print(f\"Trainable Parameters: {num_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d45fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Confusion Matrix\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Training Curves\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.legend(); plt.title(\"Loss Curve\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accs, label=\"Train Acc\")\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.legend(); plt.title(\"Accuracy Curve\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
