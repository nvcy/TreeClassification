{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a5959c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # DGCNN: Direct 3D Point Cloud Classification\n",
    "# - FPS sampling\n",
    "# - Augmentation\n",
    "# - Training, Evaluation, Metrics, and Visualization\n",
    "\n",
    "# %% Imports & Setup\n",
    "import os, math, time, random\n",
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../Utils')  # adjust if your models/configs are elsewhere\n",
    "import configs\n",
    "from models import DGCNN\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Directories\n",
    "TRAIN_SET = configs.TRAIN_DIR\n",
    "TEST_SET = configs.TEST_DIR\n",
    "MODELS_SAVE_DIR = configs.MODEL_DIR\n",
    "\n",
    "# Config\n",
    "CFG = SimpleNamespace(\n",
    "    num_points=1024,\n",
    "    k=20,\n",
    "    emb_dims=1024,\n",
    "    dropout=0.5,\n",
    "    batch_size=16,\n",
    "    epochs=80,\n",
    "    patience=12,\n",
    "    base_lr=1e-3,\n",
    "    min_lr=1e-5,\n",
    "    weight_decay=1e-4,\n",
    "    label_smoothing=0.1,\n",
    "    num_workers=4,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Seed\n",
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "set_seed(CFG.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7775da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # FPS Sampling, Normalization, and Augmentations\n",
    "\n",
    "# %%\n",
    "def farthest_point_sampling(points: np.ndarray, n_samples: int) -> np.ndarray:\n",
    "    N = points.shape[0]\n",
    "    if N == 0:\n",
    "        return np.zeros((n_samples, 3), dtype=np.float32)\n",
    "    n_samples = min(n_samples, N)\n",
    "    centroids = np.zeros((n_samples,), dtype=np.int64)\n",
    "    distances = np.ones((N,), dtype=np.float32) * 1e10\n",
    "    farthest = np.random.randint(0, N)\n",
    "    for i in range(n_samples):\n",
    "        centroids[i] = farthest\n",
    "        centroid = points[farthest]\n",
    "        dist = np.sum((points - centroid) ** 2, axis=1)\n",
    "        sel = dist < distances\n",
    "        distances[sel] = dist[sel]\n",
    "        farthest = np.argmax(distances)\n",
    "    sampled = points[centroids]\n",
    "    if sampled.shape[0] < CFG.num_points:\n",
    "        pad_idx = np.random.choice(len(sampled), CFG.num_points - len(sampled), replace=True)\n",
    "        sampled = np.concatenate([sampled, sampled[pad_idx]], axis=0)\n",
    "    return sampled.astype(np.float32)\n",
    "\n",
    "def normalize_unit(points: np.ndarray) -> np.ndarray:\n",
    "    c = points.mean(axis=0, keepdims=True)\n",
    "    p = points - c\n",
    "    scale = np.max(np.sqrt((p**2).sum(1))) + 1e-8\n",
    "    return (p / scale).astype(np.float32)\n",
    "\n",
    "def random_jitter(points: np.ndarray, sigma=0.01, clip=0.05) -> np.ndarray:\n",
    "    noise = np.clip(sigma * np.random.randn(*points.shape), -clip, clip).astype(np.float32)\n",
    "    return (points + noise).astype(np.float32)\n",
    "\n",
    "def random_scale(points: np.ndarray, low=0.85, high=1.2) -> np.ndarray:\n",
    "    s = np.random.uniform(low, high)\n",
    "    return (points * s).astype(np.float32)\n",
    "\n",
    "def random_rotation_z(points: np.ndarray) -> np.ndarray:\n",
    "    theta = np.random.uniform(0, 2*np.pi)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]], dtype=np.float32)\n",
    "    return (points @ R.T).astype(np.float32)\n",
    "\n",
    "def load_txt_points(path: Path) -> np.ndarray:\n",
    "    pts = np.loadtxt(path).astype(np.float32)\n",
    "    if pts.ndim == 1: pts = pts[None, :]\n",
    "    if pts.shape[1] > 3: pts = pts[:, :3]\n",
    "    return pts\n",
    "\n",
    "def list_point_files(root: Path):\n",
    "    files = []\n",
    "    for ext in (\"*.pts\", \"*.xyz\", \"*.txt\"):\n",
    "        files.extend(sorted((root).rglob(ext)))\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a970cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Dataset & DataLoader\n",
    "\n",
    "# %%\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, root_dir: Path, num_points=1024, augment=False):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.num_points = num_points\n",
    "        self.augment = augment\n",
    "        self.classes = sorted([d.name for d in self.root_dir.iterdir() if d.is_dir()])\n",
    "        self.class_to_idx = {c:i for i,c in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "        for cls in self.classes:\n",
    "            for f in list_point_files(self.root_dir/cls):\n",
    "                self.samples.append((f, self.class_to_idx[cls]))\n",
    "        if len(self.samples) == 0:\n",
    "            raise RuntimeError(f\"No point files found under {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        pts = load_txt_points(path)\n",
    "        pts = farthest_point_sampling(pts, self.num_points)\n",
    "        pts = normalize_unit(pts)\n",
    "        if self.augment:\n",
    "            pts = random_rotation_z(pts)\n",
    "            pts = random_scale(pts)\n",
    "            pts = random_jitter(pts)\n",
    "        return torch.from_numpy(pts.T), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Create Datasets and Loaders\n",
    "train_ds = PointCloudDataset(TRAIN_SET, num_points=CFG.num_points, augment=True)\n",
    "test_ds  = PointCloudDataset(TEST_SET,  num_points=CFG.num_points, augment=False)\n",
    "\n",
    "num_classes = len(train_ds.classes)\n",
    "print(f\"Classes ({num_classes}): {train_ds.classes}\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True,\n",
    "                          num_workers=CFG.num_workers, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=CFG.batch_size, shuffle=False,\n",
    "                          num_workers=CFG.num_workers, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ef2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Model, Optimizer, Scheduler, Loss\n",
    "\n",
    "# %%\n",
    "# Model\n",
    "model = DGCNN(num_classes=num_classes, k=CFG.k, emb_dims=CFG.emb_dims, dropout=CFG.dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss with label smoothing\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=CFG.label_smoothing)\n",
    "\n",
    "# Optimizer (AdamW for stability)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.base_lr, weight_decay=CFG.weight_decay)\n",
    "\n",
    "# Cosine Annealing LR Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=CFG.epochs, eta_min=CFG.min_lr\n",
    ")\n",
    "\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Optimizer: AdamW | Scheduler: CosineAnnealingLR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d6cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Training and Evaluation Functions\n",
    "\n",
    "# %%\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    for points, labels in loader:\n",
    "        points, labels = points.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(points)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "    return total_loss / total_samples, total_correct / total_samples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    for points, labels in loader:\n",
    "        points, labels = points.to(device), labels.to(device)\n",
    "        outputs = model(points)\n",
    "        loss = criterion(outputs, labels)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    return (\n",
    "        total_loss / total_samples,\n",
    "        total_correct / total_samples,\n",
    "        np.array(all_labels),\n",
    "        np.array(all_preds)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ed3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Training Loop with Early Stopping and History Tracking\n",
    "\n",
    "# %%\n",
    "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "best_model_wts = None\n",
    "\n",
    "for epoch in range(CFG.epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc, val_labels, val_preds = evaluate(model, test_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"[Epoch {epoch+1:02d}/{CFG.epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | \"\n",
    "          f\"Time: {elapsed:.1f}s\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_wts = model.state_dict()\n",
    "        patience_counter = 0\n",
    "        torch.save(best_model_wts, MODELS_SAVE_DIR / \"best_dgcnn.pth\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= CFG.patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "if best_model_wts is not None:\n",
    "    model.load_state_dict(best_model_wts)\n",
    "print(f\"Best Val Accuracy: {best_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf07b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Final Evaluation and Classification Metrics\n",
    "\n",
    "# %%\n",
    "val_loss, val_acc, val_labels, val_preds = evaluate(model, test_loader, criterion)\n",
    "print(f\"Final Test Loss: {val_loss:.4f} | Final Test Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=train_ds.classes))\n",
    "\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(num_classes), train_ds.classes, rotation=45)\n",
    "plt.yticks(np.arange(num_classes), train_ds.classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa86460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Training History Visualization\n",
    "\n",
    "# %%\n",
    "epochs_ran = len(train_losses)\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(1, epochs_ran+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs_ran+1), val_losses, label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(1, epochs_ran+1), train_accs, label='Train Acc')\n",
    "plt.plot(range(1, epochs_ran+1), val_accs, label='Val Acc')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training & Validation Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
