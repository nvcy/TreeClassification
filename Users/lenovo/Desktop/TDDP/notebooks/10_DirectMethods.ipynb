{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50043bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b3e13b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Running on CPU (no CUDA used)\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Cell 1: Imports & Config\n",
    "# =======================\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score,\n",
    "    f1_score, precision_score, confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import custom model & configs\n",
    "import sys\n",
    "sys.path.append(\"../Utils\")\n",
    "from models import DGCNN  # DGCNN & PointNet implementation\n",
    "import configs\n",
    "\n",
    "# Paths\n",
    "TRAIN_DIR = Path(configs.TRAIN_DIR)\n",
    "TEST_DIR = Path(configs.TEST_DIR)\n",
    "MODEL_DIR = Path(configs.MODEL_DIR)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Force CPU device (no CUDA)\n",
    "class CFG:\n",
    "    num_points = 1024\n",
    "    batch_size = 16\n",
    "    epochs = 20\n",
    "    lr = 1e-3\n",
    "    device = torch.device(\"cpu\")  # ‚úÖ FORCE CPU\n",
    "    k = 20\n",
    "    emb_dims = 1024\n",
    "    dropout = 0.5\n",
    "    num_classes = len(os.listdir(TRAIN_DIR))\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"‚ö†Ô∏è Running on CPU (no CUDA used)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "389bbbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Cell 2: Dataset & Utils\n",
    "# =======================\n",
    "def load_point_file_safe(file_path):\n",
    "    \"\"\"Load a point cloud file robustly with multiple delimiters.\"\"\"\n",
    "    for delim in [None, \" \", \",\", \"\\t\"]:\n",
    "        try:\n",
    "            pc = np.loadtxt(file_path, delimiter=delim)\n",
    "            if pc.ndim == 1:\n",
    "                pc = pc[np.newaxis, :]\n",
    "            return pc\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise ValueError(f\"Could not read {file_path}\")\n",
    "\n",
    "def normalize_unit(pc):\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    return pc / m\n",
    "\n",
    "def farthest_point_sampling(pc, n_points):\n",
    "    N, _ = pc.shape\n",
    "    if N <= n_points:\n",
    "        return np.pad(pc, ((0, n_points - N), (0, 0)))\n",
    "    centroids = np.zeros((n_points,))\n",
    "    distance = np.ones(N) * 1e10\n",
    "    farthest = np.random.randint(0, N)\n",
    "    for i in range(n_points):\n",
    "        centroids[i] = farthest\n",
    "        dist = np.sum((pc - pc[farthest])**2, axis=1)\n",
    "        distance = np.minimum(distance, dist)\n",
    "        farthest = np.argmax(distance)\n",
    "    return pc[centroids.astype(np.int32)]\n",
    "\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, root_dir, num_points=1024):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.num_points = num_points\n",
    "        self.files, self.labels = [], []\n",
    "        for i, class_dir in enumerate(sorted(self.root_dir.iterdir())):\n",
    "            if class_dir.is_dir():\n",
    "                for f in class_dir.glob(\"*.*\"):\n",
    "                    self.files.append(f)\n",
    "                    self.labels.append(i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            pc = load_point_file_safe(self.files[idx])\n",
    "            if pc.shape[1] > 3:\n",
    "                pc = pc[:, :3]\n",
    "            pc = normalize_unit(pc)\n",
    "            pc = farthest_point_sampling(pc, self.num_points)\n",
    "            return torch.from_numpy(pc.T.astype(np.float32)), int(self.labels[idx])\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"‚ö†Ô∏è Error loading idx={idx}: {e}\")\n",
    "            return torch.zeros((3, self.num_points), dtype=torch.float32), 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4fa2e65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:   502\n",
      "Validation samples: 55\n",
      "Test samples:       134\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Cell 3: DataLoaders\n",
    "# =======================\n",
    "train_dataset = PointCloudDataset(TRAIN_DIR, num_points=CFG.num_points)\n",
    "test_dataset = PointCloudDataset(TEST_DIR, num_points=CFG.num_points)\n",
    "\n",
    "val_size = int(0.1 * len(train_dataset))\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# ‚úÖ pin_memory removed (no CUDA)\n",
    "train_loader = DataLoader(train_subset, batch_size=CFG.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=CFG.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples:   {len(train_subset)}\")\n",
    "print(f\"Validation samples: {len(val_subset)}\")\n",
    "print(f\"Test samples:       {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ff08cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Model parameters: 1,801,095\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Cell 4: Model Setup\n",
    "# =======================\n",
    "model = DGCNN(CFG, output_channels=CFG.num_classes).to(CFG.device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG.lr)\n",
    "\n",
    "print(f\"Device: {CFG.device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f1ad5be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Cell 5: Train/Eval Functions\n",
    "# =======================\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    for points, labels in loader:\n",
    "        points, labels = points.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(points)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * points.size(0)\n",
    "        total_correct += preds.argmax(1).eq(labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "    return total_loss / total_samples, total_correct / total_samples\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for points, labels in loader:\n",
    "            points, labels = points.to(device), labels.to(device)\n",
    "            preds = model(points)\n",
    "            loss = criterion(preds, labels)\n",
    "            total_loss += loss.item() * points.size(0)\n",
    "            total_correct += preds.argmax(1).eq(labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.argmax(1).cpu().numpy())\n",
    "    return total_loss / total_samples, total_correct / total_samples, np.array(all_labels), np.array(all_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "35d5a2d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(CFG\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m     11\u001b[0m     epoch_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 13\u001b[0m     tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     val_loss, val_acc, val_labels, val_preds \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, CFG\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     16\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(tr_loss)\n",
      "Cell \u001b[1;32mIn[70], line 10\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m      8\u001b[0m points, labels \u001b[38;5;241m=\u001b[39m points\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 10\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(preds, labels)\n\u001b[0;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\Desktop\\TDDP\\notebooks\\../Utils\\models.py:127\u001b[0m, in \u001b[0;36mDGCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    126\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 127\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mget_graph_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m    129\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\Desktop\\TDDP\\notebooks\\../Utils\\models.py:40\u001b[0m, in \u001b[0;36mget_graph_feature\u001b[1;34m(x, k, idx)\u001b[0m\n\u001b[0;32m     37\u001b[0m     idx \u001b[38;5;241m=\u001b[39m knn(x, k\u001b[38;5;241m=\u001b[39mk)   \u001b[38;5;66;03m# (batch_size, num_points, k)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m idx_base \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mnum_points\n\u001b[0;32m     42\u001b[0m idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m+\u001b[39m idx_base\n\u001b[0;32m     44\u001b[0m idx \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:363\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m     )\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    367\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Cell 6: Training Loop\n",
    "# =======================\n",
    "best_val_acc = 0\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "start_time = time.time()\n",
    "patience, patience_counter = 5, 0\n",
    "\n",
    "for epoch in range(CFG.epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, CFG.device)\n",
    "    val_loss, val_acc, val_labels, val_preds = evaluate(model, val_loader, criterion, CFG.device)\n",
    "\n",
    "    train_losses.append(tr_loss)\n",
    "    train_accs.append(tr_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"val_acc\": val_acc\n",
    "        }, MODEL_DIR / \"best_model.pth\")\n",
    "        print(f\"‚úì New best model saved (Val Acc: {val_acc:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{CFG.epochs}] | \"\n",
    "          f\"Train Loss: {tr_loss:.4f} | Train Acc: {tr_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n",
    "          f\"Epoch Time: {(time.time()-epoch_start):.1f}s\")\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"‚ö†Ô∏è Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "exec_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Training finished in {exec_time:.1f}s | Best Val Acc: {best_val_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc, test_labels, test_preds = evaluate(model, test_loader, criterion, CFG.device)\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ea02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Cell 7: Metrics & Plots\n",
    "# =======================\n",
    "overall_acc = accuracy_score(val_labels, val_preds)\n",
    "balanced_acc = balanced_accuracy_score(val_labels, val_preds)\n",
    "f1 = f1_score(val_labels, val_preds, average=\"weighted\")\n",
    "prec_per_class = precision_score(val_labels, val_preds, average=None)\n",
    "\n",
    "print(\"üìä Validation Metrics:\")\n",
    "print(f\"Accuracy:          {overall_acc:.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "print(f\"F1 Score:          {f1:.4f}\")\n",
    "print(f\"Precision/Class:   {prec_per_class}\")\n",
    "\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.legend(); plt.title(\"Loss Curve\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label=\"Train Acc\")\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.legend(); plt.title(\"Accuracy Curve\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
